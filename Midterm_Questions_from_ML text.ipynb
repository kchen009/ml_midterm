{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 30 non-trivial multiple choice problems, 5 from each chapter 1-6. Each question should have 4 options. None of the options should be obviously correct or obviously wrong. The question should test an idea in the material. Each question should be for a different sub-section. You should have at least 5 questions for each chapter. None of the questions should come from the intro section of any chapter. Each student is makeing one post in this forum. Before you submit, please look at others posts. You should not repeat a question someone else posted. Since you can edit, you may post your chapter 1 questions, then add chapter 2, etc. so that you don't have to go back and re-edit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1) Who published the first concept of the Perceptron learning rule?\n",
    "- a) Frank Rosenblatt \n",
    "- b) McColloch-Pitts\n",
    "- c) Bernard Widrow\n",
    "- d) Neil deGrasse Tyson\n",
    "\n",
    "correct answer: a "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2) The Adaptive Linear Neuron (Adaline), a single-layer neural network, helps lay the groundwork for understanding more advanced supervised machine learning algorithms by\n",
    "- a) defining and minimizing continuous cost functions \n",
    "- b) speeding up computations with vectorized operations\n",
    "- c) shuffling of training data for stochastic gradient descent\n",
    "- d) feature scaling \n",
    "\n",
    "correct answer: a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3) The key difference between the Adaline rule and Perceptron rule is that:\n",
    "- a) Adaline requires fewers epochs for the training error to converge. \n",
    "- b) the weights are updated based on a linear activation function rather than a unit step function like in the perceptron.\n",
    "- c) The accuracy score of the Perceptron is usually higher than Adaline\n",
    "- d) Feature scaling can be applied in Adaline \n",
    "\n",
    "correct answer: b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4) The key difference between stochastic gradient descent (SGD) and gradient descent (or batch gradient descent) is that:\n",
    "- a) In stochastic gradient descent, the weights are updated incrementally for each training sample, rather than updating the weights on basis of accumulated errors over all samples.\n",
    "- b) Batch gradient descent converges much quicker than SGD. \n",
    "- c) SGD is more computationally expensive than Batch gradient descent.\n",
    "- d) There are no differences. \n",
    "\n",
    "correct answer: a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5) Which of the following is **not** true about **online learning** in SGD?\n",
    "- a) Model is trained on the fly as new training data arrives\n",
    "- b) Training data can be discarded after updating the model if storage is a concern. \n",
    "- c) It is not useful technique to train over big data sets.\n",
    "- d) Online training is a common technique used when it is computationally infeasable to train over the data set.\n",
    "\n",
    "correct answer: c "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) The StandardScaler class from scikit-learn's preprocessing module\n",
    "- a) standardizes features with 0 mean and unit variance.\n",
    "- b) scales each feature by its maximum absolute value\n",
    "- c) normalizes samples individually to unit norm.\n",
    "- d) transforms features using quantile information.\n",
    "\n",
    "correct answer: a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) The **stratify** parameter from the **train_test_split** method\n",
    "- a) shuffles the data before splitting into test and training sets \n",
    "- b) groups the data by a specified categorical variable\n",
    "- c) specifies the activation function.\n",
    "- d) returns training and test subsets that have the same proportions of class labels as the input dataset.\n",
    "\n",
    "correct answer: d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Which of the following activation functions is used for Logistic Regression? \n",
    "- a) Linear activation function\n",
    "- b) Sigmoid activation function\n",
    "- c) ReLu activation function \n",
    "- d) Power function\n",
    "\n",
    "correct answer: b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) The optimization objective in Support Vector Machine is to\n",
    "- a) minimize misclassification errors\n",
    "- b) minimize the number of epochs \n",
    "- c) maximize the margin--the distance between the separating hyperplane\n",
    "- d) minimize variance\n",
    "\n",
    "correct answer: c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Kernel methods in classification are used to deal with \n",
    "- a) nonlinear classification problems\n",
    "- b) bias/variance tradeoff\n",
    "- c) streaming data\n",
    "- d) overfitting\n",
    "\n",
    "correct answer: a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Which of the following algorithm is **not** a parametric model?\n",
    "- a) Linear Support Vector Machine\n",
    "- b) Logistic Regression\n",
    "- c) Perceptron\n",
    "- d) k-nearest neighbor \n",
    "\n",
    "correct answer: d "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1) Which of the following is not one of the steps of performing PCA, as mentioned in the chapter?\n",
    "- a) Standardize the data. \n",
    "- b) Construction the covariance matrix.\n",
    "- c) Obtaining the eigenvalues and eigenvectors of the covariance matrix. \n",
    "- d) Select the eigenvectors with the lowest corresponding eigenvalues. \n",
    "\n",
    "correct answer d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2) PCA is an ---- algorithm, whereas LDA is a ---- algorithm.\n",
    "- a) unsupervised, supervised\n",
    "- b) supervised, unsupervised\n",
    "- c) high-bias, low-bias\n",
    "- d) computationally inexpensive, computational expensive\n",
    "\n",
    "correct answer: a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3) Kernal PCA is more suitable dimension reduction method than PCA and LDA for dealing with \n",
    "- a) linearly separable data\n",
    "- b) non-linearly separable data\n",
    "- c) data with a lot of noise\n",
    "- d) big data \n",
    "\n",
    "correct answer: b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
